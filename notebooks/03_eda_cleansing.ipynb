{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA + Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In D:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In D:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "from impyute.imputation.cs import fast_knn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "# %matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set sandbox_mode boolean for image building\n",
    "* if sandbox_mode = True: faster to run, but images won't be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandbox_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_mask(df):\n",
    "    type_mask = []\n",
    "    for i in df.dtypes:\n",
    "        if str(i).startswith('float') or str(i).startswith('int'): # or str(i).startswith('bool')\n",
    "            type_mask.append(True)\n",
    "        else: type_mask.append(False)\n",
    "    num_cols = list(np.array(df.columns)[type_mask])\n",
    "    other_cols = list(np.array(df.columns)[[not elem for elem in type_mask]])\n",
    "    \n",
    "    return num_cols, other_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions related to missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing(df):\n",
    "    total = df.isnull().sum()\n",
    "    percent = (df.isnull().sum()/df.isnull().count())\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['total', 'percent_missing'])\n",
    "    missing_data['percent_missing'] = missing_data['percent_missing']\n",
    "    missing_data['percent_missing'] = missing_data['percent_missing'].apply(lambda x: round(x,2))\n",
    "    \n",
    "    return missing_data\n",
    "\n",
    "def drop_missing_from_threshold(df, row_threshold, col_threshold):\n",
    "    row_count, col_count = df.shape\n",
    "    # drop columns according to threshold of missing; use mask of columns which have less missing than threshold\n",
    "    df = df.iloc[:, (df_missing['percent_missing'] < col_threshold).to_list()]\n",
    "    \n",
    "    # drop row according to threshold of missing\n",
    "    n_cols = df.shape[1]\n",
    "    df['ratio_mis'] = df.apply(lambda x: (n_cols - x.count())/n_cols, axis=1)\n",
    "    df = df[df['ratio_mis']<row_threshold]\n",
    "    df.drop(['ratio_mis'], axis=1, inplace=True)\n",
    "    \n",
    "    # count number of removals\n",
    "    row_count_new, col_count_new = df.shape\n",
    "    row_count_removal = row_count - row_count_new\n",
    "    col_count_removal = col_count - col_count_new\n",
    "    print('{} rows and {} columns were removed from database'.format(row_count_removal, col_count_removal))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_imputation(df, method = 'knn', k=30, manual_val=-1):\n",
    "    try:\n",
    "        assert method in ['knn', 'mode', 'median', -1, 'manual']\n",
    "    except AssertionError:\n",
    "        raise ValueError('error: select a correct method for imputation: [knn, mode, median, -1, manual]')\n",
    "        \n",
    "    if method == 'knn':\n",
    "        sys.setrecursionlimit(100000) #Increase the recursion limit of the OS\n",
    "        numerical_cols, other_cols = get_numerical_mask(df)\n",
    "        \n",
    "#         df =  StandardScaler().fit_transform(df) # scale for knn to work properly (it's distance based)\n",
    "\n",
    "        # start the KNN training\n",
    "        imputed_training = fast_knn(df[numerical_cols], k=30)\n",
    "\n",
    "        # retrieve column names\n",
    "        imp_cols = imputed_training.columns.to_list()\n",
    "        imputed_training.rename({imp_cols[i]: numerical_cols[i] for i in range(len(imp_cols))}, axis = 1, inplace=True)\n",
    "        df.reset_index(inplace=True)\n",
    "        other_cols.append('id')\n",
    "        df = df[other_cols].merge(imputed_training, left_index=True, right_index=True)\n",
    "        df.set_index('id', inplace=True)\n",
    "        \n",
    "    elif method == 'mode':\n",
    "        df.fillna(data.mode().iloc[0], inplace=True)\n",
    "        \n",
    "    elif method == 'median':\n",
    "        df.fillna(df.median(), inplace=True)\n",
    "\n",
    "    elif method == -1:\n",
    "        df.fillna(-1, inplace=True)\n",
    "    \n",
    "    elif method == 'manual':\n",
    "        df.fillna(manual_val, inplace=True)\n",
    "        \n",
    "    try:\n",
    "        assert df[df.isna().any(axis=1)].shape[0] == 0\n",
    "    except AssertionError:\n",
    "        raise ValueError('there are still missing values')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions related to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_standard_deviation_tol(df, tol=3):\n",
    "    #scale data for operation\n",
    "    df = pd.DataFrame(StandardScaler().fit_transform(df[numerical_cols]))\n",
    "    \n",
    "    z = np.abs(stats.zscore(df))\n",
    "    z = pd.DataFrame(z, columns = df.columns, index=df.index)\n",
    "    z.fillna(0, inplace=True)\n",
    "    for col in z.columns[2:]:\n",
    "        z = z[z[col]<tol]\n",
    "    print(\"{0:.2%} of data was removed after dealing with outliers\".format((df.shape[0]-z.shape[0])/df.shape[0]))\n",
    "    df = df.loc[z.index, :]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_isolation_forest(df, contamination=0.05):\n",
    "    clf = IsolationForest(max_samples='auto', contamination=contamination, random_state=42) # contamination='auto' or 0.05\n",
    "    clf.fit(df)\n",
    "\n",
    "    outlier_pred = clf.predict(df)\n",
    "    print('number of outliers:', np.count_nonzero(outlier_pred == -1), 'from a total of {}'.format(len(outlier_pred)))\n",
    "    print('percentage of outliers: {0:.0%}'.format(np.count_nonzero(outlier_pred == -1)/np.count_nonzero(outlier_pred == 1)))\n",
    "    \n",
    "    return outlier_pred\n",
    "\n",
    "def get_outliers(df, label, cols, method = 'isolation_forest', if_contamination = 0.05, z_tol = 3):\n",
    "\n",
    "    if method == 'isolation_forest':\n",
    "        outliers = apply_isolation_forest(df, if_contamination)\n",
    "    elif method == 'standard_deviation_tol':\n",
    "        df = apply_standard_deviation_tol(df, z_tol)\n",
    "    \n",
    "    print(len(outliers))\n",
    "    return outliers, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define paths and capture data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = os.path.join('..', 'data', '02_intermediate')\n",
    "outputs = os.path.join('..', 'data', '02_intermediate')\n",
    "reports = os.path.join('..', 'data', '06_reporting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'..\\\\data\\\\02_intermediate\\\\X_train.csv' does not exist: b'..\\\\data\\\\02_intermediate\\\\X_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-72c45e0a4ba5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m               \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'X_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_test\u001b[0m          \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'X_test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_train\u001b[0m            \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'y_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_test\u001b[0m             \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'y_test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'..\\\\data\\\\02_intermediate\\\\X_train.csv' does not exist: b'..\\\\data\\\\02_intermediate\\\\X_train.csv'"
     ]
    }
   ],
   "source": [
    "data               = pd.read_csv(os.path.join(inputs, 'X_train.csv'), index_col='id')\n",
    "data_test          = pd.read_csv(os.path.join(inputs, 'X_test.csv'), index_col='id')\n",
    "y_train            = pd.read_csv(os.path.join(inputs, 'y_train.csv'), index_col='id')\n",
    "y_test             = pd.read_csv(os.path.join(inputs, 'y_test.csv'), index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset dimensions:', data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get types of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct data types\n",
    "for c in ['preset_1', 'preset_2']:\n",
    "    data[c] = data[c].astype('object')\n",
    "    \n",
    "numerical_cols, other_cols = get_numerical_mask(data)\n",
    "numerical_cols.remove('cycle') # remove cycle as it is not important for treatment\n",
    "other_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking  for possible anomalies in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing data\n",
    "usually, missing data is solved by filling it with some metric such as median. If the number of missing data in some entry is too high, we must evaluate for the removal of those entries.\n",
    "For categorical with missing data, if you want to encode missing values, first change its type to a string:\n",
    "```python\n",
    "a[pd.isnull(a)]  = 'NaN'\n",
    "```\n",
    "Some refs:\n",
    "* https://stackoverflow.com/questions/36808434/label-encoder-encoding-missing-values\n",
    "\n",
    "About the missing values, we can't assume beforehand if those are Missing at Random (MAR) or Missing not at Random (MNAR). Further investigation would be necessary to properly decide over how to handle it.\n",
    "\n",
    "For now, I am assuming they are Missing at Random. So I will remove some of them through a threshold, and apply imputation for the rest. By applying a proper imputation I observed a slight improvement over the score.\n",
    "\n",
    "The catch is that applying imputation over euclidean distances can be extremely imprecise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop columns and rows for threshold of missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SORTED LIST OF MISSING VALUES')\n",
    "df_missing = get_missing(data)\n",
    "df_missing_vis = df_missing[df_missing['total'] > 0]\n",
    "df_missing_vis['percent_missing'] = df_missing_vis['percent_missing'].apply(lambda x: round(x, 2))\n",
    "# df_missing_vis.sort_values(by='percent_missing', ascending=False).head(20)\n",
    "df_missing_vis.sort_values(by='percent_missing', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mis_threshold = 0.8\n",
    "row_mis_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = drop_missing_from_threshold(data, row_mis_threshold, col_mis_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize rows with missing\n",
    "we already know that the critical columns are related to geo_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandbox_mode = True\n",
    "if sandbox_mode:\n",
    "    print('number of missing:', data[data.isna().any(axis=1)].shape[0])\n",
    "data[data.isna().any(axis=1)].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imputation of missing values\n",
    "For continuous values, I would prefer knn over median, but it depends on normalized dataset. Nevertheless, we don't have missing on continuous datasets, even though we could encode categorical data. But the encoding step wasn't organized to precede this notebook, so I will stick to 'mode', which imputes the most frequent value.\n",
    "\n",
    "Some refs:\n",
    "* https://jamesrledoux.com/code/imputation#:~:text=One%20approach%20to%20imputing%20categorical,given%20in%20Pandas'%20value_counts%20function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the apply_imputation function accepts the following methods: knn, median, mode, or -1 (impute as category -1 [for categorical vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputation over numerical variables\n",
    "data[numerical_cols] = data[numerical_cols].astype(float)\n",
    "if data[numerical_cols].isnull().values.any():\n",
    "    data = apply_imputation(data, method = 'knn', k = 30)\n",
    "\n",
    "# imputation over categorical variables\n",
    "if data[other_cols].isnull().values.any():\n",
    "    data[other_cols] = apply_imputation(data[other_cols], method = 'mode', k = 30)\n",
    "    \n",
    "# manual imputation on lag and forecast variables\n",
    "manual_cols = ['lag_1', 'lag_2', 'lag_3']\n",
    "if data[manual_cols].isnull().values.any():\n",
    "    data[manual_cols] = apply_imputation(data[manual_cols], method = 'manual', manual_val = False)\n",
    "# data_test['y'].fillna(value=data_test['y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['lag_1', 'lag_2', 'lag_3']:\n",
    "    data[col] = data[col].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# anomaly detection\n",
    "remove outliers from choosing one of the following methods: isolation_forest, standard_deviation_tol (using z_score on standardized version)\n",
    "\n",
    "other parameters are:\n",
    "* if_contamination: isolation forest level of contamination\n",
    "* z_tol: tolerance for standard deviation (if using zscore)\n",
    "\n",
    "It is not advisable to remove outliers without proper consideration, but I lacked time to analyse it. So I used a conservative approach to remove them (low contamination threshold = few removals)\n",
    "\n",
    "Some interesting refs:\n",
    "* https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\n",
    "* https://towardsdatascience.com/anomaly-detection-with-isolation-forest-visualization-23cd75c281e2\n",
    "* https://towardsdatascience.com/outlier-detection-with-isolation-forest-3d190448d45e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomaly_count(df, lag=10):\n",
    "    for i in range(0,df.shape[0]-lag):\n",
    "        window = df.loc[i:i+lag,:][['if_anomaly']]\n",
    "        out_count = window[window['if_anomaly'] == -1].shape[0]\n",
    "        df.loc[df.index[i+lag],'anomaly_count_lag10'] = out_count\n",
    "    \n",
    "\n",
    "    for i in range(0, lag):\n",
    "        window = df.loc[0:i,:][['if_anomaly']]\n",
    "        out_count = window[window['if_anomaly'] == -1].shape[0]\n",
    "        df.loc[df.index[i],'anomaly_count_lag10'] = out_count\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# temporarily add train to test data for lagged info (no leakage here)\n",
    "test_start = data_test.index[0]\n",
    "data_test = data.append(data_test)\n",
    "\n",
    "# get outliers on train data\n",
    "data['if_anomaly'], _ = get_outliers(data[numerical_cols], y_train, numerical_cols, \n",
    "                                                  method = 'isolation_forest', if_contamination = 0.05)\n",
    "# get column with count of preceding anomalies\n",
    "data = get_anomaly_count(data, lag=10)\n",
    "    \n",
    "# get outliers on test data (needs trainset)\n",
    "\n",
    "data_test['if_anomaly'], _ = get_outliers(data_test[numerical_cols], y_train, numerical_cols, \n",
    "                                                  method = 'isolation_forest', if_contamination = 0.05)\n",
    "data_test = get_anomaly_count(data_test, lag=10)\n",
    "\n",
    "# removes trainset again\n",
    "data_test = data_test[data_test.index >= test_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_data = data[data['if_anomaly'] == -1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ploting_dict = {'filename': 'outlier_analysis', 'xlabel': 'feature', 'ylabel': 'outlier count', \n",
    "                'title': 'number of outliers per feature'}\n",
    "\n",
    "plot_outliers(get_outlier_dict(outliers_data), ploting_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outlier removal isn't justified for this database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_outliers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_outliers:\n",
    "    data = data[data['if_anomaly'] == 1]\n",
    "    y_train = y_train[y_train.index.isin(data.index.to_list())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['if_anomaly'], axis=1, inplace=True)\n",
    "data_test.drop(['if_anomaly'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose response variable\n",
    "current y is the 'Failed' variable, but we might decide to change it to y_forecast1, which considers Failures on T+1.\n",
    "\n",
    "One of those variables must be dropped."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data['y'] = data['y_forecast1']\n",
    "data_test['y'] = data_test['y_forecast1']\n",
    "\n",
    "data.drop(['y_forecast1'], axis=1, inplace=True)\n",
    "data_test.drop(['y_forecast1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize pairwise relations\n",
    "When datasets have just a few variables (10â€“15), pairplots allow for a quick visual inspection of those relations, as well as bariable distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols, other_cols = get_numerical_mask(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = int(len(numerical_cols)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sandbox_mode:\n",
    "    data_vis = data[numerical_cols[:separator]]\n",
    "    data_vis['y'] = y_train['y']\n",
    "    print('visualize pairplots')\n",
    "    sns.pairplot(data_vis, plot_kws={'alpha': 0.1});\n",
    "    plt.savefig(os.path.join(reports,'01_pairplots_1.jpg'), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sandbox_mode:\n",
    "    data_vis = data[numerical_cols[separator:]]\n",
    "    data_vis['y'] = y_train['y']\n",
    "    print('visualize pairplots')\n",
    "    sns.pairplot(data_vis, plot_kws={'alpha': 0.1});\n",
    "    plt.savefig(os.path.join(reports,'01_pairplots_2.jpg'), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if data is imbalanced\n",
    "When data is imbalanced, we must evaluate for solutions such as oversampling or undersamplig, which might be done with techniques such as SMOTE (Synthetic Minority Oversampling Technique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_train.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = sns.color_palette()\n",
    "classif = y_train['y'].value_counts()\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(classif.index, classif.values, alpha=0.8, color=color[0])\n",
    "plt.ylabel('Number of occurrences', fontsize=12)\n",
    "plt.xlabel('Failed', fontsize=12)\n",
    "plt.plot()\n",
    "plt.savefig(os.path.join(reports,'imbalance.jpg'), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = y['y'].value_counts()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ratio between classes:',round(counts[0]/counts[1],2))\n",
    "print('\\r\\nCheck proportions below:')\n",
    "y['y'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description (2nd round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sandbox_mode:\n",
    "    print(data.shape[0])\n",
    "    print(y_train.shape[0])\n",
    "    data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(os.path.join(outputs, 'X_train.csv'))\n",
    "data_test.to_csv(os.path.join(outputs, 'X_test.csv'))\n",
    "\n",
    "y_train.to_csv(os.path.join(outputs, 'y_train.csv'))\n",
    "y_test.to_csv(os.path.join(outputs, 'y_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
